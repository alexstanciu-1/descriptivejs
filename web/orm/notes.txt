

structure
	-> on disk structure

table
	multiple files
		each file a no of record blocks
			each record blocks a no of record(s)/column based storage

# may just leave a gap , move blocks inside the file ... why not, gaps in columns
	# move record-blocks if not fitting any more, or have a gap and move it to another aligned table 



file = each record block will have a no of recordsets
		will hold storage meta | for alter and other operations
		each table chunk will hold multiple recordset column/stored
		x/rec/column | 1 = row storage

table
	cols segments
	each col segment = file (with some columns)
	data is aligned
	- no redundancy !!!
	- if we cache too much ... on data changes -> lots of stuff to do
	- no redundant caching !!!

	- commit writes need to keep cache in sync

	on column storage ... write-ahead will not have much gaps

	# recordsets with specific number of columns

	# small write => on disk
	# large writes => WAL

	# on commit -> memory - to disk - if too spread out ... we can WAL 
		# we should know the cost of the commit ! time/load wise

	# the WAL must look just like the other data, flags to specify the columns, vacuum will re-position the data

1,2,3,4,5,6,7,8
a,b,c,d,e,f,g,h

# writting on the WAL:

4,7,8
x,y,z

# indexing ?!

# on a re-boot ... it will `see` that it's not on the same 'position' as the last inserted ... and it will have to adapt
# each table(virtual) in a folder (the cost is soooo small)
# each column in a folder ?! that would be interesting ... too many writes in the WAL
# fsss ... if we write to 20 tables ... 20 files ... meeeh ... that can not happen

# WAL : recordset(table,cols_group)+data | a write-set
# repeat, index can be in memory
# so a pointer ... file -> recordset

SELECT id,name,code,active,description FROM Products 

Products > int (it's hash)
	we need to know where all the products data is stored
	first look in the WAL
	then in specific files (try to avoid redundant stuff as much as possible, while having a decent wake-up time)
		... for example ... files with 0000*** -> may be the WAL ... as the first 0000 will say ... no table (or similar)

	these data pages ... can have a prev-next linked list style + WAL

	one big file DB
	one for data ... other for indexes (or mix ... ppfffff)

	the WAL should be the last resort ... we should set data in memory

	if we calc these pages ok ... we should keep the data in some order

	# smart vaccum - pages must drop under 70% to be considered for vaccum
	# order ?! some kind of consecutive gaps ... so at least a page is released in proximity
	# if then a record is inserted in-between, add it at the end ... then ... make room for it by inserting a page
	# why not just insert a new page ... link it and fill the needed data, based on consecutive ids
	# keeping the data ordered

	# indexes ?! - he he he ... 
		# id/refs indexes (B+Tree/BRIN/hash?)
		# data indexes (B+Tree/BRIN/hash?)

	# data page -> at least 128 rec-sets

table
	split per files (enable column storage) | aligned or not | (redundant ? ... why not !)
	each file will have 'pages' , each page will have it's own layout defined (to allow incrmental structure changes)
	we add 45 new columns ... (sh't) add them a new, aligned, file first, then merge them, maybe in a new file also
		may be a lot faster ... :|
	pages to be big to worth the struct changes , but small to be refit easy if needed

table
	file
		pages (column/position)
			first -> pointer to the first page
			next/prev (pages) / to keep an order by Id

(select columns ... $c1 -> pageX, $c2 -> pageY, ...)
(pages need to stay grouped on the same ID ... somehow)

# main pages
meta: first page, last page | then on each page: prev/next, then each page will know it's group
	: c1 : g1, c2: g1, c3: g2 .... or (page's groups and layout : 3 groups, 1: c1, 2: c2 ....)
	: may also hold first/last per group ... but that would be cached info ?!

PER PAGE: group pos (so it can be recovered)

each page may have a different layout (to be able to re-fit on struct upgrades)
each page must say rowsize, isset per row ... etc

page A1 , page A2, page A3 (rowid is the same)
page B1
page C1

# references -> Category -> point to ID -> then ID to hash | may hold a physical ref ... future
#								(by config: UPDATE ON IDs should not be allowed)

index | index as needed ?!
	...

==========================================================================================================

user
====
alex
teo
mihai
geo
coro

office
======
15nov
itc
bbc
sitei
coastei

Select a.*,b.* FROM user,office


======
alex  | ian | 500
alex  | feb | 500
alex  | mar | 500
teo   | ian | 700
teo   | feb | 700
teo   | mar | 700
....

===========================================================================

data on disk === data on memory + meta/lock info ?! 
	why not atm

	how do we put it in memory ?!

	take chunk -> mem | meeeh

	data, indexes

	# sometimes we need big chunks ... 
		# sometimes we need it one by one ... unlucky

	# store in files 
		# all fixed width
			# var len can be joined , and expaned as needed
				# if not ending with zero ... next table
					# misc data in other tables

	# virtual tables

	... so only one type of storage with a config to group by cols in X size ... if X = 1 => classic row

	# virtual tables | aligned/not

	# referencez | hashes or ?!


	# memory hash != disk hash


	# SELECT * FROM Table WHERE Id IN (....)

	# we could order the ids ... look it up in a B+ tree, will it be faster 

# ordered references, ..., just find the page ...

# release data, release index chunks , reduce index size | 

# pointer : db.table.row.cell | 

# db['name']['table'][213]['code']

# [213] : [file/segment]

# context $table = db['name']['table']

# pointers/pointers/pointers

# db_1 = cursor('db_name')
# db_2 = cursor('db_name')
# ...
# cursor(db, table, 
# live db changes # drop/change ... must do a column lock, same for the table, do a table lock

# lock : table, col, row, cell

# one cursor for each column / that gives a bit better control
# some internal worker points to data in memory

# aligned by in groups 

int ids[100];
int names[100];

db* $db;
table* $table;
column* $column;
row* $row;

cursor* $cursor = new cursor($table, $row, ...);

while (int $id = $cursor->current())
{
	# then inner whatever
	string code = coll($cursor, $column_code); # this is some inline function that will give us the value, whatever

	$cursor->next()
}

the cursor will know where to point ... hopefully




	


